{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Convert to TF-TRT INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will convert a TensorFlow saved model into a TF-TRT optimized graph using INT8 precision. You will use the optimized graph to make predictions and will benchmark its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you wil be able to:\n",
    "\n",
    "- Use TF-TRT to optimize a saved model with INT8 precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_helpers import (\n",
    "    get_images, batch_input, load_tf_saved_model,\n",
    "    predict_and_benchmark_throughput_from_saved, display_prediction_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batched Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "images = get_images(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input = batch_input(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to TF-TRT INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform INT8 optimization, we simply need to:\n",
    "\n",
    "- Set `precision_mode` to `trt.TrtPrecisionMode.INT8`\n",
    "- Pass a `calibration_input_fn` to `converter.convert`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Input Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calibration_input_fn` should be a generator function that yields input data as a list or tuple.\n",
    "\n",
    "You need to make sure that the calibration dataset covers all the expected scenarios, for example, clear weather, rainy day, night scenes, etc. When examining your own dataset, you should create a separate calibration dataset. The calibration dataset should not overlap with the training, validation, or test datasets.\n",
    "\n",
    "For our simple example here, we will not take these extra steps and will simply pass in our `batched_input` as calibration data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to TF-TRT INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address the `TODO`s and make this function capable of performing conversion for INT8 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_trt_graph_and_save(precision_mode='float32', input_saved_model_dir='resnet_v2_152_saved_model', calibration_data=batched_input):\n",
    "    \n",
    "    if precision_mode == 'float32':\n",
    "        precision_mode = trt.TrtPrecisionMode.FP32\n",
    "        converted_save_suffix = '_TFTRT_FP32'\n",
    "        \n",
    "    if precision_mode == 'float16':\n",
    "        precision_mode = trt.TrtPrecisionMode.FP16\n",
    "        converted_save_suffix = '_TFTRT_FP16'\n",
    "\n",
    "    # TODO: correctly set precision_mode\n",
    "    if precision_mode == 'int8':\n",
    "        precision_mode = None\n",
    "        converted_save_suffix = '_TFTRT_INT8'\n",
    "        \n",
    "    output_saved_model_dir = input_saved_model_dir + converted_save_suffix\n",
    "    \n",
    "    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "        precision_mode=precision_mode, \n",
    "        max_workspace_size_bytes=8000000000\n",
    "    )\n",
    "\n",
    "    converter = trt.TrtGraphConverterV2(\n",
    "        input_saved_model_dir=input_saved_model_dir,\n",
    "        conversion_params=conversion_params\n",
    "    )\n",
    "\n",
    "    print('Converting {} to TF-TRT graph precision mode {}...'.format(input_saved_model_dir, precision_mode))\n",
    "    \n",
    "    if precision_mode == trt.TrtPrecisionMode.INT8:\n",
    "        \n",
    "        # Here we define a simple generator to yield calibration data\n",
    "        def calibration_input_fn():\n",
    "            yield (calibration_data, )\n",
    "\n",
    "        # TODO: When performing INT8 optimization, we must pass the named argument calibration_input_fn to converter.convert.\n",
    "        # Use the `calibration_input_fn` defined a few lines above for this.\n",
    "        converter.convert()\n",
    "    \n",
    "    else:\n",
    "        converter.convert()\n",
    "\n",
    "    print('Saving converted model to {}...'.format(output_saved_model_dir))\n",
    "    converter.save(output_saved_model_dir=output_saved_model_dir)\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to check your work. Takes a couple minutes.\n",
    "convert_to_trt_graph_and_save(precision_mode='int8', input_saved_model_dir='resnet_v2_152_saved_model', calibration_data=batched_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the next cell to see the solution if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_trt_graph_and_save(precision_mode='float32', input_saved_model_dir='resnet_v2_152_saved_model', calibration_data=batched_input):\n",
    "    \n",
    "    if precision_mode == 'float32':\n",
    "        precision_mode = trt.TrtPrecisionMode.FP32\n",
    "        converted_save_suffix = '_TFTRT_FP32'\n",
    "        \n",
    "    if precision_mode == 'float16':\n",
    "        precision_mode = trt.TrtPrecisionMode.FP16\n",
    "        converted_save_suffix = '_TFTRT_FP16'\n",
    "\n",
    "    # Per usual, set the precision_mode\n",
    "    if precision_mode == 'int8':\n",
    "        precision_mode = trt.TrtPrecisionMode.INT8\n",
    "        converted_save_suffix = '_TFTRT_INT8'\n",
    "        \n",
    "    output_saved_model_dir = input_saved_model_dir + converted_save_suffix\n",
    "    \n",
    "    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "        precision_mode=precision_mode, \n",
    "        max_workspace_size_bytes=8000000000\n",
    "    )\n",
    "\n",
    "    converter = trt.TrtGraphConverterV2(\n",
    "        input_saved_model_dir=input_saved_model_dir,\n",
    "        conversion_params=conversion_params\n",
    "    )\n",
    "\n",
    "    print('Converting {} to TF-TRT graph precision mode {}...'.format(input_saved_model_dir, precision_mode))\n",
    "    \n",
    "    if precision_mode == trt.TrtPrecisionMode.INT8:\n",
    "        \n",
    "        # Here we define a simple generator to yield calibration data\n",
    "        def calibration_input_fn():\n",
    "            yield (calibration_data, )\n",
    "\n",
    "        # When performing INT8 optimization, we must pass a calibration function to convert\n",
    "        converter.convert(calibration_input_fn=calibration_input_fn)\n",
    "    \n",
    "    else:\n",
    "        converter.convert()\n",
    "\n",
    "    print('Saving converted model to {}...'.format(output_saved_model_dir))\n",
    "    converter.save(output_saved_model_dir=output_saved_model_dir)\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark TF-TRT INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the optimized TF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = load_tf_saved_model('resnet_v2_152_saved_model_TFTRT_INT8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference with the optimized graph, and after a warmup, time and calculate throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = predict_and_benchmark_throughput_from_saved(batched_input, infer, N_warmup_run=50, N_run=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to view predictions, which you can use for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_run_preds = all_preds[0]\n",
    "display_prediction_info(last_run_preds, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute the cell below to restart the kernel and clear GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook you will optimize additional models, and experiment with the impact of changing the `minimum_segment_size` conversion parameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
