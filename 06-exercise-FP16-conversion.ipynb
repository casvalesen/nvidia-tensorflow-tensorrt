{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Convert to TF-TRT Float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you'll update the `convert_to_trt_graph_and_save` function you worked with in the last notebook to be able to also perform conversion for Float16 precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "\n",
    "- Optimize a saved model with TF-TRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_helpers import (\n",
    "    get_images, batch_input, load_tf_saved_model,\n",
    "    predict_and_benchmark_throughput_from_saved, display_prediction_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batched Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these cells to create batched input. You don't need to modify the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_images = 32\n",
    "images = get_images(number_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input = batch_input(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address the `TODO` and make this function capable of performing conversion for Float16 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_trt_graph_and_save(precision_mode='float32', input_saved_model_dir='resnet_v2_152_saved_model'):\n",
    "    \n",
    "    if precision_mode == 'float32':\n",
    "        precision_mode = trt.TrtPrecisionMode.FP32\n",
    "        converted_save_suffix = '_TFTRT_FP32'\n",
    "        \n",
    "    if precision_mode == 'float16':\n",
    "        # TODO: Correctly set precision_mode\n",
    "        precision_mode = None\n",
    "        converted_save_suffix = '_TFTRT_FP16'\n",
    "        \n",
    "    output_saved_model_dir = input_saved_model_dir + converted_save_suffix\n",
    "    \n",
    "    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "        precision_mode=precision_mode, \n",
    "        max_workspace_size_bytes=8000000000\n",
    "    )\n",
    "\n",
    "    converter = trt.TrtGraphConverterV2(\n",
    "        input_saved_model_dir=input_saved_model_dir,\n",
    "        conversion_params=conversion_params\n",
    "    )\n",
    "\n",
    "    print('Converting {} to TF-TRT graph precision mode {}...'.format(input_saved_model_dir, precision_mode))\n",
    "\n",
    "    converter.convert()\n",
    "\n",
    "    print('Saving converted model to {}...'.format(output_saved_model_dir))\n",
    "    converter.save(output_saved_model_dir=output_saved_model_dir)\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_trt_graph_and_save(precision_mode='float16', input_saved_model_dir='resnet_v2_152_saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the next cell to see the solution if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```python\n",
    "def convert_to_trt_graph_and_save(precision_mode='float32', input_saved_model_dir='resnet_v2_152_saved_model'):\n",
    "    \n",
    "    if precision_mode == 'float32':\n",
    "        precision_mode = trt.TrtPrecisionMode.FP32\n",
    "        converted_save_suffix = '_TFTRT_FP32'\n",
    "        \n",
    "    if precision_mode == 'float16':\n",
    "        # TODO: Correctly set precision_mode`\n",
    "        precision_mode = trt.TrtPrecisionMode.FP16\n",
    "        converted_save_suffix = '_TFTRT_FP16'\n",
    "        \n",
    "    output_saved_model_dir = input_saved_model_dir + converted_save_suffix\n",
    "    \n",
    "    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "        precision_mode=precision_mode, \n",
    "        max_workspace_size_bytes=8000000000\n",
    "    )\n",
    "\n",
    "    converter = trt.TrtGraphConverterV2(\n",
    "        input_saved_model_dir=input_saved_model_dir,\n",
    "        conversion_params=conversion_params\n",
    "    )\n",
    "\n",
    "    print('Converting {} to TF-TRT graph precision mode {}...'.format(input_saved_model_dir, precision_mode))\n",
    "\n",
    "    converter.convert()\n",
    "\n",
    "    print('Saving converted model to {}...'.format(output_saved_model_dir))\n",
    "    converter.save(output_saved_model_dir=output_saved_model_dir)\n",
    "    print('Complete')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark TF-TRT Float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the optimized TF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = load_tf_saved_model('resnet_v2_152_saved_model_TFTRT_FP16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference with the optimized graph, and after a warmup, time and calculate throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = predict_and_benchmark_throughput_from_saved(batched_input, infer, N_warmup_run=50, N_run=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to view predictions, which you can use for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_run_preds = all_preds[0]\n",
    "display_prediction_info(last_run_preds, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute the cell below to restart the kernel and clear GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you'll learn about the additional steps required to optimize TF-TRT models with Int8 precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
