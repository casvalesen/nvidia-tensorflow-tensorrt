{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to TF-TRT Float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how to convert a TensorFlow saved model into a TF-TRT optimized graph using Float32 precision. We will use the optimized graph to make predictions and will benchmark its performance. In the next notebook, you will be asked to make your first optimized TF-TRT graph using Float16 precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- Convert a saved TensorFlow model into an optimized TF-TRT graph with Float32 precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of TensorFlow 2.0, TRT is integrated into Tensorflow under the `tensorflow.python.compiler` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebook, we will rely on several helper functions. If needed, please use the file menu on the left hand side of the JupyterLab environment to open and inspect `./lab_helpers.py` for more details about the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_helpers import (\n",
    "    get_images, batch_input, load_tf_saved_model,\n",
    "    predict_and_benchmark_throughput_from_saved, display_prediction_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batched Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebook, we will create a batched input of many images to send to the GPU for inference at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_images = 32\n",
    "images = get_images(number_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input = batch_input(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`convert_to_trt_graph_and_save` expects the directory of a saved model, which it will convert to an optimized TF-TRT graph with Float32 precision, and then save. Please read the comments for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_trt_graph_and_save(input_saved_model_dir='resnet_v2_152_saved_model'):\n",
    "    \n",
    "    precision_mode = trt.TrtPrecisionMode.FP32\n",
    "    converted_save_suffix = '_TFTRT_FP32'\n",
    "        \n",
    "    output_saved_model_dir = input_saved_model_dir + converted_save_suffix\n",
    "    \n",
    "    # Here we overwrite the default conversion parameters to suit our needs.\n",
    "    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "        precision_mode=precision_mode, \n",
    "        max_workspace_size_bytes=8000000000\n",
    "    )\n",
    "\n",
    "    # Trt.GraphConverterV2 takes the saved model and conversion parameters, and returns a TF-TRT converter.\n",
    "    converter = trt.TrtGraphConverterV2(\n",
    "        input_saved_model_dir=input_saved_model_dir,\n",
    "        conversion_params=conversion_params\n",
    "    )\n",
    "\n",
    "    print('Converting {} to TF-TRT graph precision mode {}...'.format(input_saved_model_dir, 'float32'))\n",
    "    \n",
    "    # converter.convert() performs the optimization.\n",
    "    converter.convert()\n",
    "\n",
    "    print('Saving converted model to {}...'.format(output_saved_model_dir))\n",
    "    \n",
    "    # converter.save will save the model as a TF (not Keras) saved-model at the specified directory.\n",
    "    converter.save(output_saved_model_dir=output_saved_model_dir)\n",
    "    \n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_trt_graph_and_save(input_saved_model_dir='resnet_v2_152_saved_model') # Takes about a minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark TF-TRT Float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the optimized TF model. Note that this is a TF saved model, as opposed to a Keras saved model. If you wish, refer to `lab_helpers.py` for details on the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = load_tf_saved_model('resnet_v2_152_saved_model_TFTRT_FP32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform inference with the optimized graph, and after a warmup, time and calculate throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = predict_and_benchmark_throughput_from_saved(batched_input, infer, N_warmup_run=50, N_run=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare *Throughput* to the naive TF 2 inference perfomed earlier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to view predictions, which you can use to compare to the naive TF 2 run. You should see very little difference in the accuracy of the predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_run_preds = all_preds[0]\n",
    "display_prediction_info(last_run_preds, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute the cell below to restart the kernel and clear GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, you will be asked to make your first optimized TF-TRT graph using Float16 precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
